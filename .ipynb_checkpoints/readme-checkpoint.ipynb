{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "不同模型对中文文本进行分类\n",
    "\n",
    "\n",
    "## 数据集  \n",
    "\n",
    "\n",
    "- THUCNews 是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。使用 THUCTC 工具包在此数据集上进行评测，准确率可以达到88.6%。\n",
    "\n",
    "**本文使用 THUCNews 的一个子集进行训练与测试，使用了其中的 10 个分类，每个分类 6500 条数据。**\n",
    "\n",
    "下载地址: https://pan.baidu.com/s/1hugrfRu 密码: qfud\n",
    "\n",
    "类别如下：\n",
    "\n",
    "体育, 财经, 房产, 家居, 教育, 科技, 时尚, 时政, 游戏, 娱乐\n",
    "\n",
    "数据集划分如下：\n",
    "\n",
    "训练集: 5000 \\* 10  \n",
    "验证集: 500 \\* 10  \n",
    "测试集: 1000 \\* 10  \n",
    "\n",
    "具体介绍请参考：[text-classification-cnn-rnn](https://github.com/gaussic/text-classification-cnn-rnn)  \n",
    "\n",
    "## 分类效果  \n",
    "\n",
    "### 1、tensorflow\n",
    "- char-based  \n",
    "\n",
    "| model      |fasttext |   cnn   |   rnn   |  rcnn   |   han   |  dpcnn  |  bert   |\n",
    "|:-----      | :-----: | :-----: | :-----: | :-----: | :-----  | :-----: | :-----: |\n",
    "| val_acc    |  92.92  |  94.98  |  93.56  |  95.14  |  95.10  |  94.80  |  97.84  |\n",
    "| test_acc   |  93.15  |  95.90  |  94.37  |  95.35  |  94.15  |  95.66  |  96.93  |\n",
    "\n",
    "- bert 用的 vocab 比别的模型大，且经过预训练，直接比较不公平\n",
    "\n",
    "- word-based  \n",
    "\n",
    "| model      |fasttext |   cnn   |   rnn   |  rcnn   |   han   |  dpcnn  |  bert   |\n",
    "|:-----      | :-----: | :-----: | :-----: | :-----: | :-----  | :-----: | :-----: |\n",
    "| val_acc    |  95.52  |  95.28  |         |  95.60  |  95.10  |  95.68  |    -    |\n",
    "| test_acc   |  95.34  |  95.77  |         |  96.36  |  95.66  |  95.97  |    -    |\n",
    "\n",
    "\n",
    "### 2、sklearn\n",
    "\n",
    "- word_frequency-based\n",
    "\n",
    "| model      |   SVC   |LinearSVC|   LR    | RandomForest |GradientBoost|\n",
    "|:-----      | :-----: | :-----: | :-----: |    :-----:   |   :-----    |\n",
    "| val_acc    |  88.34  |  90.30  |  91.30  |     92.40    |    91.22    |\n",
    "\n",
    "- word_tfidf-based\n",
    "\n",
    "| model      |   SVC   |LinearSVC|   LR    | RandomForest |GradientBoost|\n",
    "|:-----      | :-----: | :-----: | :-----: |    :-----:   |   :-----    |\n",
    "| val_acc    |  18.08  |  90.86  |  90.80  |     92.82    |    90.24    |\n",
    "\n",
    "\n",
    "## 模型介绍  \n",
    "\n",
    "### 1、FastText  \n",
    "\n",
    "fasttext_model.py 文件为训练和测试 fasttext 模型的代码\n",
    "\n",
    "![图1 FastText 模型结构图](images/fasttext.jpg?raw=true)\n",
    "\n",
    "本代码简化了 fasttext 模型的结构，模型结构非常简单，运行速度简直飞快，模型准确率也不错，可根据实际需要优化模型结构\n",
    "\n",
    "### 2、TextCNN  \n",
    "\n",
    "cnn_model.py 文件为训练和测试 TextCNN 模型的代码\n",
    "\n",
    "![图2 TextCNN 模型结构图](images/textcnn.jpg?raw=true)\n",
    "\n",
    "本代码实现了 TextCNN 模型的结构，通过 3 个不同大小的卷积核，对输入文本进一维卷积，分别 pooling 三个卷积之后的 feature， 拼接到一起，然后进行 dense 操作，最终输出模型结果。可实现速度和精度之间较好的折中。\n",
    "\n",
    "### 3、RNN\n",
    "\n",
    "rnn_model.py 文件为训练和测试 TextCNN 模型的代码\n",
    "\n",
    "![图8 TextCNN 模型结构图](images/textrnn.jpg?raw=true)\n",
    "\n",
    "本代码实现了 TextRNN 模型的结构，对输入序列进行embedding，然后输入两层的 rnn_cell中学习序列特征，取最后一个 word 的 state 作为进行后续的 fc 操作，最终输出模型结果。 \n",
    "\n",
    "### 4、RCNN  \n",
    "\n",
    "rcnn_model.py 文件为训练和测试 RCNN 模型的代码\n",
    "\n",
    "![图3 RCNN 模型结构图](images/rcnn.jpg?raw=true)\n",
    "\n",
    "[Recurrent Convolutional Neural Network for Text Classification](https://scholar.google.com.hk/scholar?q=Recurrent+Convolutional+Neural+Networks+for+Text+Classification&hl=zhCN&as_sdt=0&as_vis=1&oi=scholart&sa=X&ved=0ahUKEwjpx82cvqTUAhWHspQKHUbDBDYQgQMIITAA), 在学习 word representations 时候，同时采用了 rnn 结构来学习 word 的上下文，虽然模型名称为 RCNN，但并没有显式的存在卷积操作。\n",
    "\n",
    "1、采用双向lstm学习 word 的上下文\n",
    "\n",
    "   c_left = tf.concat([tf.zeros(shape), output_fw[:, :-1]], axis=1, name=\"context_left\")\n",
    "   c_right = tf.concat([output_bw[:, 1:], tf.zeros(shape)], axis=1, name=\"context_right\")\n",
    "   word_representation = tf.concat([c_left, embedding_inputs, c_right], axis=2, name=\"last\")\n",
    "\n",
    "2、pooling + softmax\n",
    "\n",
    "  word_representation  的维度是 batch_size * seq_length * 2 * context_dim + embedding_dim\n",
    "  在 seq_length 维度进行 max pooling，然后进行 fc 操作就可以进行分类了，可以将该网络看成是 fasttext 的改进版本\n",
    "\n",
    "\n",
    "### 5、HAN  \n",
    "\n",
    "han_model.py 文件为训练和测试 HAN 模型的代码\n",
    "\n",
    "![图4 HAN 模型结构图](images/han.jpg?raw=true)\n",
    "\n",
    "HAN 为 Hierarchical Attention Networks，将待分类文本，分为一定数量的句子，分别在 word level 和 sentence level 进行 encoder 和 attention 操作，从而实现对较长文本的分类。  \n",
    "\n",
    "本文是按照句子长度将文本分句的，实际操作中可按照标点符号等进行分句，理论上效果能好一点。  \n",
    "\n",
    "- 1、对文本进行分句  \n",
    "  \n",
    "  \n",
    "  对每个句子进行双向lstm编码，得到 $h = [\\overrightarrow h, \\overleftarrow h]$  \n",
    "  \n",
    "  batch_size = 64, seq_length = 600, sent_num = 10, emb_size = 128, lstm_hid_dim = 256\n",
    "  \n",
    "  数据维度变化：$64 * 600 * 128 --- (64*10） * 60 * 128 --- (64*10） * 60 * 512$\n",
    "\n",
    "\n",
    "- 2、word level attention  \n",
    "\n",
    "\n",
    "$ u = tanh(W*h + b) \\tag{1}$  \n",
    "\n",
    "u 可以看做是 h 的 hidden representation， $shape(u) = (64*10） * 60 * 256$  \n",
    "\n",
    "$ \\alpha = \\frac {e^{u^T * u_w}}{\\sum_t {e^{u^T * u_w}}} \\tag{2}$  \n",
    "\n",
    "$u_w(1 * 256)$ 是一个 word level context vector,通过对该向量与上面的 hidden representation 之间的相似性进行 softmax，得到每个单词在句子中的权重, $shape(\\alpha) = 60 * 1$  \n",
    "\n",
    "$ s = \\sum_t {\\alpha h} \\tag{3}$  \n",
    "\n",
    "对输入的状态进行加权求和，得到句子的向量表示\n",
    "\n",
    "数据维度变化：$(64*10） * 60 * 512 --- (64*10） * 512$  \n",
    "\n",
    "\n",
    "- 3、得到每个句子的向量表示  \n",
    "\n",
    "- 4、sentence level attention  \n",
    "\n",
    "  与 word level attention 过程一样，只是该层是句子级别的attention  \n",
    "  \n",
    "  数据维度变化：$64 * 10 * 512 --- 64 * 512$\n",
    "\n",
    "- 5、得到 document 的向量表示  \n",
    "\n",
    "- 6、dence + softmax  \n",
    "\n",
    "\n",
    "### 6、DPCNN  \n",
    "\n",
    "dpcnn_model.py 文件为训练和测试 DPCNN 模型的代码  \n",
    "\n",
    "![图5 DPCNN 模型结构图](images/dpcnn.jpg?raw=true)\n",
    "\n",
    "DPCNN 通过卷积和残差连接增加了以往用于文本分类 CNN 网络的深度，可以有效提取文本中的远程关系特征，并且复杂度不高。  \n",
    "\n",
    "- region_embedding: word_embedding 之后进行的 ngram 卷积结果\n",
    "\n",
    "\n",
    "### 7、BERT  \n",
    "\n",
    "bert_model.py 文件为训练和测试 BERT 模型的代码  \n",
    "\n",
    "google官方提供用于文本分类的demo写的比较抽象，所以本文基于 google 提供的代码和初始化模型，重写了文本分类模型的训练和测试代码，bert 分类模型在小数据集下效果很好，通过较少的迭代次数就能得到很好的效果，但是训练和测试速度较慢，这点不如基于 CNN 的网络结构。  \n",
    "\n",
    "bert_model.py 将训练数据和验证数据存储为 tfrecord 文件，然后进行训练  \n",
    "\n",
    "由于 bert 提供的预训练模型较大，需要自己去 [google-research/bert](https://github.com/google-research/bert) 中下载预训练好的模型，本实验采用的是 \"BERT-Base, Chinese\" 模型。\n",
    "\n",
    "![图6 BERT 输入数据格式](images/bert_1.jpeg?raw=true)\n",
    "\n",
    "![图7 BERT 下游任务介绍](images/bert_2.jpeg?raw=true)\n",
    "\n",
    "## 参考  \n",
    "\n",
    "- 1 [text-classification-cnn-rnn](https://github.com/gaussic/text-classification-cnn-rnn)  \n",
    "- 2 [text_classification](https://github.com/brightmart/text_classification)  \n",
    "- 3 [bert](https://github.com/google-research/bert)  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
